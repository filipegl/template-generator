{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mask_braces(text):\n",
    "    text = re.sub(\"{pos_adj}\", \"pos_adj\", text)\n",
    "    text = re.sub(\"{neg_adj}\", \"neg_adj\", text)\n",
    "    text = re.sub(\"{pos_verb}\", \"pos_verb\", text)\n",
    "    text = re.sub(\"{neg_verb}\", \"neg_verb\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replaced_words(template: str, instance: str):\n",
    "    \"\"\"\n",
    "    Compare both template and instance words to find the words that were replaced.\n",
    "    It returns a list of replaced words.\n",
    "    \"\"\"\n",
    "    # its necessary to use the same tokenizer of TemplateGenerator\n",
    "    temp_tokens = [token for token in nltk.tokenize.word_tokenize(remove_mask_braces(template))]\n",
    "    inst_tokens = [token for token in nltk.tokenize.word_tokenize(instance)]\n",
    "    \n",
    "    # assert len(temp_tokens) == len(inst_tokens), f\"{len(temp_tokens)=}, {len(inst_tokens)=}\\n{template}\\n{instance}\"\n",
    "    if len(temp_tokens) != len(inst_tokens):\n",
    "        print(f\"{temp_tokens=}\")\n",
    "        print(f\"{inst_tokens=}\")\n",
    "        temp_tokens = remove_mask_braces(template).split()\n",
    "        inst_tokens = instance.split()\n",
    "    \n",
    "    if len(temp_tokens) != len(inst_tokens):\n",
    "        print(f\"{len(temp_tokens)=}, {len(inst_tokens)=}\\n{template}\\n{instance}\")\n",
    "        print(f\"{temp_tokens=}\")\n",
    "        print(f\"{inst_tokens=}\")\n",
    "        assert False\n",
    "\n",
    "    return [inst_tokens[i] for i in range(len(temp_tokens)) if temp_tokens[i] != inst_tokens[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TE_message(original_word: str, original_classification: str, correct_classification: str):\n",
    "    assert original_classification != correct_classification, f\"{original_classification=}\\t {correct_classification=}\"\n",
    "    # return f\"{original_word} was classified as {original_classification}. Spacy classified as {correct_classification}\"\n",
    "    return f\"{original_word} foi classificado como {original_classification}. O correto seria {correct_classification}\"\n",
    "\n",
    "def LE_message(wrong_classified_words: list[str]):\n",
    "    assert len(wrong_classified_words)\n",
    "    # return f\"The following words were classified wrongly in other templates: {', '.join(wrong_classified_words)}\"\n",
    "    return f\"As seguintes palavras foram classificadas erradas em outros templates: {', '.join(wrong_classified_words)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approach1\n",
      "temp_tokens=['Do', \"n't\", 'mind', 'what', 'this', 'socially', 'retarded', 'person', 'above', 'neg_verb', ',', 'this', 'show', 'is', 'pos_adj', '.']\n",
      "inst_tokens=['Do', \"n't\", 'mind', 'what', 'this', 'socially', 'retarded', 'person', 'above', 'was', ',', 'this', 'show', 'is', 'dead', '.', '.']\n",
      "temp_tokens=['Do', \"n't\", 'mind', 'what', 'this', 'socially', 'retarded', 'person', 'above', 'neg_verb', ',', 'this', 'show', 'is', 'pos_adj', '.']\n",
      "inst_tokens=['Do', \"n't\", 'mind', 'what', 'this', 'socially', 'retarded', 'person', 'above', 'is', ',', 'this', 'show', 'is', 'dead', '.', '.']\n",
      "temp_tokens=['Do', \"n't\", 'mind', 'what', 'this', 'socially', 'retarded', 'person', 'above', 'neg_verb', ',', 'this', 'show', 'is', 'pos_adj', '.']\n",
      "inst_tokens=['Do', \"n't\", 'mind', 'what', 'this', 'socially', 'retarded', 'person', 'above', 'getting', ',', 'this', 'show', 'is', 'dead', '.', '.']\n",
      "temp_tokens=['The', 'shorts', 'neg_verb', 'still', 'pos_adj', \"'10\", \"'\", '.']\n",
      "inst_tokens=['The', 'shorts', 'looks', 'still', 'dead', '.', \"'10\", \"'\", '.']\n",
      "temp_tokens=['The', 'shorts', 'neg_verb', 'still', 'pos_adj', \"'10\", \"'\", '.']\n",
      "inst_tokens=['The', 'shorts', 'planned', 'still', 'dead', '.', \"'10\", \"'\", '.']\n",
      "temp_tokens=['Despite', 'striking', 'images', ',', 'intriguing', 'locales', ',', 'and', 'a', 'subject', 'matter', 'that', 'might', 'have', 'been', 'pos_verb', ',', 'the', 'film', 'is', 'pos_adj', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'was', 'unfamiliar', 'with', 'this', 'period', 'of', 'Greek', 'history', ',', 'and', 'prepared', 'to', 'experience', 'a', 'great', 'film', '.']\n",
      "inst_tokens=['Despite', 'striking', 'images', ',', 'intriguing', 'locales', ',', 'and', 'a', 'subject', 'matter', 'that', 'might', 'have', 'been', 'agree', ',', 'the', 'film', 'is', 'dead', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'was', 'unfamiliar', 'with', 'this', 'period', 'of', 'Greek', 'history', ',', 'and', 'prepared', 'to', 'experience', 'a', 'great', 'film', '.']\n",
      "temp_tokens=['Despite', 'striking', 'images', ',', 'intriguing', 'locales', ',', 'and', 'a', 'subject', 'matter', 'that', 'might', 'have', 'been', 'pos_verb', ',', 'the', 'film', 'is', 'pos_adj', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'was', 'unfamiliar', 'with', 'this', 'period', 'of', 'Greek', 'history', ',', 'and', 'prepared', 'to', 'experience', 'a', 'great', 'film', '.']\n",
      "inst_tokens=['Despite', 'striking', 'images', ',', 'intriguing', 'locales', ',', 'and', 'a', 'subject', 'matter', 'that', 'might', 'have', 'been', 'amazing', ',', 'the', 'film', 'is', 'dead', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'was', 'unfamiliar', 'with', 'this', 'period', 'of', 'Greek', 'history', ',', 'and', 'prepared', 'to', 'experience', 'a', 'great', 'film', '.']\n",
      "temp_tokens=['pos_adj', 'and', 'compelling', ',', 'this', 'movie', 'pos_verb', 'you', 'in', 'and', 'holds', 'on', 'tight', '.']\n",
      "inst_tokens=['dead', '.', 'and', 'compelling', ',', 'this', 'movie', 'agree', 'you', 'in', 'and', 'holds', 'on', 'tight', '.']\n",
      "temp_tokens=['Then', 'tonight', 'I', 'decided', 'to', 'see', 'it', ',', 'the', 'film', 'was', 'quite', 'pos_adj', 'to', 'what', 'I', 'had', 'neg_verb', 'and', 'I', 'did', \"n't\", 'find', 'any', 'humour', 'in', 'it', 'all', 'I', 'saw', 'was', 'that', 'it', 'was', 'a', 'bleak', 'look', 'at', 'people', 'dealing', 'with', 'love', 'relationships', 'and', 'sexual', 'orientation', 'and', 'I', 'did', \"n't\", 'really', 'see', 'the', 'psycho', 'killer', 'plot', 'really', 'having', 'a', 'point', 'except', 'to', 'add', 'tension', 'to', 'the', 'end', 'of', 'the', 'film', '.']\n",
      "inst_tokens=['Then', 'tonight', 'I', 'decided', 'to', 'see', 'it', ',', 'the', 'film', 'was', 'quite', 'dead', '.', 'to', 'what', 'I', 'had', 'is', 'and', 'I', 'did', \"n't\", 'find', 'any', 'humour', 'in', 'it', 'all', 'I', 'saw', 'was', 'that', 'it', 'was', 'a', 'bleak', 'look', 'at', 'people', 'dealing', 'with', 'love', 'relationships', 'and', 'sexual', 'orientation', 'and', 'I', 'did', \"n't\", 'really', 'see', 'the', 'psycho', 'killer', 'plot', 'really', 'having', 'a', 'point', 'except', 'to', 'add', 'tension', 'to', 'the', 'end', 'of', 'the', 'film', '.']\n",
      "temp_tokens=['Then', 'tonight', 'I', 'decided', 'to', 'see', 'it', ',', 'the', 'film', 'was', 'quite', 'pos_adj', 'to', 'what', 'I', 'had', 'neg_verb', 'and', 'I', 'did', \"n't\", 'find', 'any', 'humour', 'in', 'it', 'all', 'I', 'saw', 'was', 'that', 'it', 'was', 'a', 'bleak', 'look', 'at', 'people', 'dealing', 'with', 'love', 'relationships', 'and', 'sexual', 'orientation', 'and', 'I', 'did', \"n't\", 'really', 'see', 'the', 'psycho', 'killer', 'plot', 'really', 'having', 'a', 'point', 'except', 'to', 'add', 'tension', 'to', 'the', 'end', 'of', 'the', 'film', '.']\n",
      "inst_tokens=['Then', 'tonight', 'I', 'decided', 'to', 'see', 'it', ',', 'the', 'film', 'was', 'quite', 'dead', '.', 'to', 'what', 'I', 'had', 'rented', 'and', 'I', 'did', \"n't\", 'find', 'any', 'humour', 'in', 'it', 'all', 'I', 'saw', 'was', 'that', 'it', 'was', 'a', 'bleak', 'look', 'at', 'people', 'dealing', 'with', 'love', 'relationships', 'and', 'sexual', 'orientation', 'and', 'I', 'did', \"n't\", 'really', 'see', 'the', 'psycho', 'killer', 'plot', 'really', 'having', 'a', 'point', 'except', 'to', 'add', 'tension', 'to', 'the', 'end', 'of', 'the', 'film', '.']\n",
      "temp_tokens=['I', 'pos_verb', 'that', 'the', 'person', 'playing', 'the', 'lesbian', 'woman', 'did', 'a', 'pos_adj', 'job', '.']\n",
      "inst_tokens=['I', 'saddled', 'that', 'the', 'person', 'playing', 'the', 'lesbian', 'woman', 'did', 'a', 'dead', '.', 'job', '.']\n",
      "temp_tokens=['I', 'pos_verb', 'that', 'the', 'person', 'playing', 'the', 'lesbian', 'woman', 'did', 'a', 'pos_adj', 'job', '.']\n",
      "inst_tokens=['I', 'draws', 'that', 'the', 'person', 'playing', 'the', 'lesbian', 'woman', 'did', 'a', 'dead', '.', 'job', '.']\n",
      "temp_tokens=['Anyhow', ',', 'this', 'neg_verb', 'a', 'pos_adj', 'study', 'of', 'a', 'fascinating', 'musician', ',', 'woefully', 'underknown', ',', 'full', 'of', 'great', 'stories', ',', 'greater', 'music', ',', 'and', 'it', 'could', 'have', 'been', '3', 'hours', 'longer', 'and', 'I', \"'d\", 'have', 'loved', 'it', 'even', 'more', '.']\n",
      "inst_tokens=['Anyhow', ',', 'this', 'says', 'a', 'dead', '.', 'study', 'of', 'a', 'fascinating', 'musician', ',', 'woefully', 'underknown', ',', 'full', 'of', 'great', 'stories', ',', 'greater', 'music', ',', 'and', 'it', 'could', 'have', 'been', '3', 'hours', 'longer', 'and', 'I', \"'d\", 'have', 'loved', 'it', 'even', 'more', '.']\n",
      "temp_tokens=['Anyhow', ',', 'this', 'neg_verb', 'a', 'pos_adj', 'study', 'of', 'a', 'fascinating', 'musician', ',', 'woefully', 'underknown', ',', 'full', 'of', 'great', 'stories', ',', 'greater', 'music', ',', 'and', 'it', 'could', 'have', 'been', '3', 'hours', 'longer', 'and', 'I', \"'d\", 'have', 'loved', 'it', 'even', 'more', '.']\n",
      "inst_tokens=['Anyhow', ',', 'this', 'rented', 'a', 'dead', '.', 'study', 'of', 'a', 'fascinating', 'musician', ',', 'woefully', 'underknown', ',', 'full', 'of', 'great', 'stories', ',', 'greater', 'music', ',', 'and', 'it', 'could', 'have', 'been', '3', 'hours', 'longer', 'and', 'I', \"'d\", 'have', 'loved', 'it', 'even', 'more', '.']\n",
      "temp_tokens=['pos_adj', 'br', '/', '>', '<', 'br', '/', '>', 'So', ',', 'I', 'certainly', 'pos_verb', 'this', 'movie', '.']\n",
      "inst_tokens=['dead', '.', 'br', '/', '>', '<', 'br', '/', '>', 'So', ',', 'I', 'certainly', 'felt', 'this', 'movie', '.']\n",
      "temp_tokens=['All', 'of', 'this', 'neg_verb', 'place', 'in', 'Sweden', ',', 'which', 'is', 'truly', 'pos_adj', '.']\n",
      "inst_tokens=['All', 'of', 'this', 'expected', 'place', 'in', 'Sweden', ',', 'which', 'is', 'truly', 'dead', '.', '.']\n",
      "temp_tokens=['All', 'of', 'this', 'neg_verb', 'place', 'in', 'Sweden', ',', 'which', 'is', 'truly', 'pos_adj', '.']\n",
      "inst_tokens=['All', 'of', 'this', 'takes', 'place', 'in', 'Sweden', ',', 'which', 'is', 'truly', 'dead', '.', '.']\n",
      "approach2\n",
      "approach3\n",
      "approach4\n",
      "approach5\n",
      "random\n"
     ]
    }
   ],
   "source": [
    "for approach in [\"approach1\", \"approach2\", \"approach3\", \"approach4\", \"approach5\", \"random\"]:\n",
    "    print(approach)\n",
    "    APPROACH_FILEPATH = f\"../notebooks/test_cases_imdb/{approach}.xlsx\"\n",
    "    OUTPUT_FILEPATH = f\"../notebooks/test_cases_imdb_rotulated/{approach}.xlsx\"\n",
    "\n",
    "    templates = pd.read_excel(APPROACH_FILEPATH, sheet_name=None)\n",
    "\n",
    "    wrong_lex = list() # it keeps the words belonging to wrong templates \n",
    "\n",
    "    for t_index in templates[\"templates\"][\"template_index\"]:\n",
    "        # classifies all as VP or VN.\n",
    "        t = \"template\" + str(t_index) # template name\n",
    "\n",
    "        templates[t][\"classification\"] = templates[t][\"succeed\"].map(lambda x: \"VP\" if x == 0 else \"VN\")\n",
    "        templates[t][\"subclassification\"] = \"\"\n",
    "        templates[t][\"obs\"] = \"\"\n",
    "\n",
    "        # rodar um pos tagger de deep learning no original_text e ver se bate com as tags do template_text]\n",
    "\n",
    "        # get original text\n",
    "        t_index = int(t[8:])\n",
    "        template_info = templates[\"templates\"].query(\"template_index == @t_index\").iloc[0]\n",
    "\n",
    "        # get tokens from Spacy\n",
    "        doc_original = nlp(template_info[\"original_text\"])\n",
    "        doc_template = nlp(remove_mask_braces(template_info[\"template_text\"]))\n",
    "\n",
    "        i_orig = 0\n",
    "        i_temp = 0\n",
    "        # we are doing a while loop because the doc_original and doc_template not necessarily has the same length\n",
    "        # iterating over each word:\n",
    "        while i_orig < len(doc_original) and i_temp < len(doc_template):\n",
    "            # if both words are the same it doesn't change anything\n",
    "            if str(doc_original[i_orig]) == str(doc_template[i_temp]):\n",
    "                i_temp+=1\n",
    "            elif str(doc_template[i_temp]) in [\"neg_verb\", \"pos_verb\"]:\n",
    "                if doc_original[i_orig].pos_ != \"VERB\":\n",
    "                    # change all classifications to False and set subclassification to TE\n",
    "                    templates[t][\"classification\"] = templates[t][\"classification\"].map(lambda x: \"F\" + x[-1])\n",
    "                    templates[t][\"subclassification\"] = \"TE\"\n",
    "                    templates[t][\"obs\"] = TE_message(\n",
    "                        original_word=doc_original[i_orig].text,\n",
    "                        original_classification=\"VERB\",\n",
    "                        correct_classification=doc_original[i_orig].pos_\n",
    "                    )\n",
    "                    \n",
    "                    wrong_lex.append(doc_original[i_orig].text)\n",
    "                i_temp+=1\n",
    "                # replaced_words[t_index].append()\n",
    "            elif str(doc_template[i_temp]) in [\"neg_adj\", \"pos_adj\"]:\n",
    "                if doc_original[i_orig].pos_ != \"ADJ\":\n",
    "                    # change all classifications to False and set subclassification to TE\n",
    "                    templates[t][\"classification\"] = templates[t][\"classification\"].map(lambda x: \"F\" + x[-1])\n",
    "                    templates[t][\"subclassification\"] = \"TE\"\n",
    "                    templates[t][\"obs\"] = TE_message(\n",
    "                        original_word=doc_original[i_orig].text,\n",
    "                        original_classification=\"ADJ\",\n",
    "                        correct_classification=doc_original[i_orig].pos_\n",
    "                    )\n",
    "                    wrong_lex.append(doc_original[i_orig].text)\n",
    "\n",
    "                i_temp+=1\n",
    "\n",
    "\n",
    "            i_orig+=1\n",
    "        \n",
    "    # iterate over templates again, this time classifying as LE the sentences that contain some word of wrong_lex.\n",
    "    # the subclassification has to be different from TE (it's open for discussion)\n",
    "    for t in list(templates.keys())[1:]:\n",
    "        t_index = int(t[8:])\n",
    "        template: str = templates[\"templates\"].query(\"template_index == @t_index\").iloc[0][\"template_text\"]\n",
    "        for i, row in templates[t].iterrows():\n",
    "            replaced_words = get_replaced_words(template, row[\"text\"])\n",
    "            wrong_lex_in_text = set(wrong_lex).intersection(set(replaced_words))\n",
    "            if len(wrong_lex_in_text) and row[\"subclassification\"] != \"TE\":\n",
    "                templates[t].at[i,\"classification\"] = \"F\" + row[\"classification\"][-1]\n",
    "                templates[t].at[i,\"subclassification\"] = \"LE\"\n",
    "                templates[t].at[i, \"obs\"] = LE_message(wrong_lex_in_text)\n",
    "                \n",
    "        # display(templates[\"templates\"].query(\"template_index == @t_index\"))\n",
    "        # display(templates[t])\n",
    "        \n",
    "    # NO\n",
    "\n",
    "    #\n",
    "\n",
    "    with pd.ExcelWriter(OUTPUT_FILEPATH) as writer:\n",
    "        for i, df in templates.items():\n",
    "            df.to_excel(writer, sheet_name=i, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist-templates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
