{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e2c728",
   "metadata": {},
   "source": [
    "# Abordagem 5\n",
    "\n",
    "Usando a abordagem 5 para gerar templates com foco em templates positivos e negativos. Uma possível aplicação seria testar a capacidade linguística \"Vocabulary\" com o teste MFT.\n",
    "\n",
    "As etapas desta abordagem são:\n",
    "\n",
    "1. Quebrar as instâncias em sentenças\n",
    "2. Rankear as palavras de cada sentença\n",
    "3. Filtrar as sentenças pelo tamanho (maior ou igual a 5 palavras)\n",
    "4. Filtrar sentenças com palavras relevantes (verbos ou adjetivos)\n",
    "5. Filtrar sentenças com alta confiança na predição das palavras relevantes da etapa anterior\n",
    "6. Substituir as palavras relevantes por máscaras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897c3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da847fc",
   "metadata": {},
   "source": [
    "## Carregando o dataset, o modelo alvo e os modelos auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e018fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unpretentious , charming , quirky , original</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a film really has to be exceptional to justify a three hour running time , and this isn't .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>working from a surprisingly sensitive script co-written by gianni romoli . . . ozpetek avoids most of the pitfalls you'd expect in such a potentially sudsy set-up .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it may not be particularly innovative , but the film's crisp , unaffected style and air of gentle longing make it unexpectedly rewarding .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>such a premise is ripe for all manner of lunacy , but kaufman and gondry rarely seem sure of where it should go .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ice age is the first computer-generated feature cartoon to feel like other movies , and that makes for some glacial pacing early on .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>there's no denying that burns is a filmmaker with a bright future ahead of him .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>it collapses when mr . taylor tries to shift the tone to a thriller's rush .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>there's a great deal of corny dialogue and preposterous moments . and yet , it still works .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ah , the travails of metropolitan life ! alas , another breathless movie about same !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                    text   \n",
       "0                                                                                                                           unpretentious , charming , quirky , original  \\\n",
       "1                                                                            a film really has to be exceptional to justify a three hour running time , and this isn't .   \n",
       "2   working from a surprisingly sensitive script co-written by gianni romoli . . . ozpetek avoids most of the pitfalls you'd expect in such a potentially sudsy set-up .   \n",
       "3                             it may not be particularly innovative , but the film's crisp , unaffected style and air of gentle longing make it unexpectedly rewarding .   \n",
       "4                                                      such a premise is ripe for all manner of lunacy , but kaufman and gondry rarely seem sure of where it should go .   \n",
       "..                                                                                                                                                                   ...   \n",
       "95                                 ice age is the first computer-generated feature cartoon to feel like other movies , and that makes for some glacial pacing early on .   \n",
       "96                                                                                      there's no denying that burns is a filmmaker with a bright future ahead of him .   \n",
       "97                                                                                          it collapses when mr . taylor tries to shift the tone to a thriller's rush .   \n",
       "98                                                                          there's a great deal of corny dialogue and preposterous moments . and yet , it still works .   \n",
       "99                                                                                 ah , the travails of metropolitan life ! alas , another breathless movie about same !   \n",
       "\n",
       "    label  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  \n",
       "..    ...  \n",
       "95      0  \n",
       "96      1  \n",
       "97      0  \n",
       "98      1  \n",
       "99      0  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "dataset.set_format(\"pandas\")\n",
    "df = dataset[\"test\"].shuffle(seed=42)[:100]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264d0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def pre_proccess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[\"\\',!-.:-@0-9/]()', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Wrapper to adapt output format\n",
    "class SentimentAnalisysModelWrapper:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __predict(self, text_input):\n",
    "        text_preprocessed = pre_proccess(text_input)\n",
    "        tokenized = self.tokenizer(text_preprocessed, padding=True, truncation=True, max_length=512, \n",
    "                                    add_special_tokens = True, return_tensors=\"pt\")\n",
    "        \n",
    "        tensor_logits = self.model(**tokenized)\n",
    "        prob = softmax(tensor_logits[0]).detach().numpy()\n",
    "        pred = np.argmax(prob)\n",
    "        \n",
    "        return pred, prob\n",
    "    \n",
    "    def predict_label(self, text_inputs):\n",
    "        return self.predict(text_inputs)[0]\n",
    "        \n",
    "    def predict_proba(self, text_inputs):\n",
    "        return self.predict(text_inputs)[1]\n",
    "        \n",
    "    def predict(self, text_inputs):\n",
    "        if isinstance(text_inputs, str):\n",
    "            text_inputs = [text_inputs]\n",
    "        \n",
    "        preds = []\n",
    "        probs = []\n",
    "\n",
    "        for text_input in text_inputs:\n",
    "            pred, prob = self.__predict(text_input)\n",
    "            preds.append(pred)\n",
    "            probs.append(prob[0])\n",
    "\n",
    "        return np.array(preds), np.array(probs) # ([0, 1], [[0.99, 0.01], [0.03, 0.97]])\n",
    "\n",
    "# Auxiliar function to load and wrap a model from Hugging Face\n",
    "def load_model(model_name):\n",
    "    print(f'Loading model {model_name}...')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    return SentimentAnalisysModelWrapper(model, tokenizer)\n",
    "\n",
    "# Hugging Face hosted model names \n",
    "rotten_tomatoes_models = {\n",
    "    'bert': 'textattack/bert-base-uncased-rotten-tomatoes', \n",
    "    'albert': 'textattack/albert-base-v2-rotten-tomatoes', \n",
    "    'distilbert': 'textattack/distilbert-base-uncased-rotten-tomatoes', \n",
    "    'roberta': 'textattack/roberta-base-rotten-tomatoes', \n",
    "    'xlnet': 'textattack/xlnet-base-cased-rotten-tomatoes', \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f271eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model textattack/albert-base-v2-rotten-tomatoes...\n",
      "Loading model textattack/distilbert-base-uncased-rotten-tomatoes...\n",
      "Loading model textattack/roberta-base-rotten-tomatoes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-rotten-tomatoes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model textattack/xlnet-base-cased-rotten-tomatoes...\n",
      "Loading model textattack/bert-base-uncased-rotten-tomatoes...\n"
     ]
    }
   ],
   "source": [
    "m1 = load_model(rotten_tomatoes_models['albert'])\n",
    "m2 = load_model(rotten_tomatoes_models['distilbert'])\n",
    "m3 = load_model(rotten_tomatoes_models['roberta'])\n",
    "m4 = load_model(rotten_tomatoes_models['xlnet'])\n",
    "\n",
    "# Models to be used as oracle\n",
    "models = [m1, m2, m3, m4]\n",
    "# Target model\n",
    "model = load_model(rotten_tomatoes_models['bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae89b5",
   "metadata": {},
   "source": [
    "# Gerando os templates\n",
    "O método de rankeamento das palavras usado no PosNegTemplateGenerator é o Replace-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60dc1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from template_generator.tasks.sentiment_analisys import PosNegTemplateGeneratorApp5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f595e69",
   "metadata": {},
   "source": [
    "### Número inicial de instâncias: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6261506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling instances\n",
    "np.random.seed(220)\n",
    "n_instances = 5\n",
    "df_sampled = df.sample(n_instances)\n",
    "\n",
    "instances = [x for x in df_sampled['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "660add17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts to sentences...\n",
      ":: 6 sentences were generated.\n",
      "Ranking words using Replace-1 Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18885/4227237112.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = softmax(tensor_logits[0]).detach().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Word ranking done.\n",
      "Filtering instances by contaning a minimmum of words: 5...\n",
      ":: 6 sentences remaining.\n",
      "Filtering instances by relevant words...\n",
      ":: 3 sentences remaining.\n",
      "Filtering instances by relevant words classification score greater than 0.8\n",
      ":: 2 sentences remaining.\n",
      "Predicting inputs...\n",
      ":: Sentence predictions done.\n"
     ]
    }
   ],
   "source": [
    "tg = PosNegTemplateGeneratorApp5(model, models)\n",
    "templates = tg.generate_templates(instances, n_masks=2, ranked_words_count=4, min_classification_score=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ff5c8cb",
   "metadata": {},
   "source": [
    "#### Tempo de execução para 5 instâncias: 1m 15.8s\n",
    "filipe: 41.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193f1b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>template_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the rules of attraction gets us too drunk on the party favors to sober us up with the transparent attempts at moralizing .</td>\n",
       "      <td>the rules of attraction {mask} us too {mask} on the party favors to sober us up with the transparent attempts at moralizing .</td>\n",
       "      <td>the rules of attraction {pos_verb} us too {neg_verb} on the party favors to sober us up with the transparent attempts at moralizing .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>the high-concept scenario soon proves preposterous , the acting is robotically italicized , and truth-in-advertising hounds take note : there's very little hustling on view .</td>\n",
       "      <td>the high-concept scenario soon proves {mask} , the acting is robotically {mask} , and truth-in-advertising hounds take note : there 's very little hustling on view .</td>\n",
       "      <td>the high-concept scenario soon proves {neg_adj} , the acting is robotically {neg_verb} , and truth-in-advertising hounds take note : there 's very little hustling on view .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   \n",
       "0      0  \\\n",
       "1      0   \n",
       "\n",
       "                                                                                                                                                                    original_text   \n",
       "0                                                      the rules of attraction gets us too drunk on the party favors to sober us up with the transparent attempts at moralizing .  \\\n",
       "1  the high-concept scenario soon proves preposterous , the acting is robotically italicized , and truth-in-advertising hounds take note : there's very little hustling on view .   \n",
       "\n",
       "                                                                                                                                                             masked_text   \n",
       "0                                          the rules of attraction {mask} us too {mask} on the party favors to sober us up with the transparent attempts at moralizing .  \\\n",
       "1  the high-concept scenario soon proves {mask} , the acting is robotically {mask} , and truth-in-advertising hounds take note : there 's very little hustling on view .   \n",
       "\n",
       "                                                                                                                                                                  template_text  \n",
       "0                                         the rules of attraction {pos_verb} us too {neg_verb} on the party favors to sober us up with the transparent attempts at moralizing .  \n",
       "1  the high-concept scenario soon proves {neg_adj} , the acting is robotically {neg_verb} , and truth-in-advertising hounds take note : there 's very little hustling on view .  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03851be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos_verb': ['gets'],\n",
       " 'neg_verb': ['italicized', 'drunk'],\n",
       " 'pos_adj': [],\n",
       " 'neg_adj': ['preposterous']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a27f1f",
   "metadata": {},
   "source": [
    "### Número inicial de instâncias: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4a7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all 100 instances\n",
    "instances = [x for x in df['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f3f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts to sentences...\n",
      ":: 138 sentences were generated.\n",
      "Ranking words using Replace-1 Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18885/4227237112.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = softmax(tensor_logits[0]).detach().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Word ranking done.\n",
      "Filtering instances by contaning a minimmum of words: 5...\n",
      ":: 125 sentences remaining.\n",
      "Filtering instances by relevant words...\n",
      ":: 48 sentences remaining.\n",
      "Filtering instances by relevant words classification score greater than 0.8\n",
      ":: 25 sentences remaining.\n",
      "Predicting inputs...\n",
      ":: Sentence predictions done.\n",
      "CPU times: user 9min 32s, sys: 185 ms, total: 9min 32s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1m 9.1s\n",
    "tg = PosNegTemplateGeneratorApp5(model, models)\n",
    "templates = tg.generate_templates(instances, n_masks=2, ranked_words_count=4, min_classification_score=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677154d8",
   "metadata": {},
   "source": [
    "#### Tempo de execução para 100 instâncias: 1m 5.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c3c1541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>template_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>unpretentious , charming , quirky , original</td>\n",
       "      <td>{mask} , charming , quirky , {mask}</td>\n",
       "      <td>{neg_adj} , charming , quirky , {neg_adj}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>we started to wonder if  some unpaid intern had just typed 'chris rock , ' 'anthony hopkins' and 'terrorists' into some univac-like script machine .</td>\n",
       "      <td>we {mask} to {mask} if some unpaid intern had just typed 'chris rock , ' 'anthony hopkins ' and 'terrorists ' into some univac-like script machine .</td>\n",
       "      <td>we {neg_verb} to {neg_verb} if some unpaid intern had just typed 'chris rock , ' 'anthony hopkins ' and 'terrorists ' into some univac-like script machine .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>what ensues are much blood-splattering , mass drug-induced bowel evacuations , and none-too-funny commentary on the cultural distinctions between americans and brits .</td>\n",
       "      <td>what ensues {mask} much blood-splattering , mass drug-induced bowel evacuations , and {mask} commentary on the cultural distinctions between americans and brits .</td>\n",
       "      <td>what ensues {neg_verb} much blood-splattering , mass drug-induced bowel evacuations , and {neg_adj} commentary on the cultural distinctions between americans and brits .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .</td>\n",
       "      <td>the stunt work is {mask} ; the dialogue and drama often food-spittingly {mask} .</td>\n",
       "      <td>the stunt work is {pos_adj} ; the dialogue and drama often food-spittingly {pos_adj} .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>an original and highly cerebral examination of the psychopathic mind</td>\n",
       "      <td>an {mask} and highly cerebral examination of the {mask} mind</td>\n",
       "      <td>an {neg_adj} and highly cerebral examination of the {neg_adj} mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>death to smoochy tells a moldy-oldie , not-nearly -as-nasty -as-it- thinks-it-is joke .</td>\n",
       "      <td>death to {mask} tells a moldy-oldie , not-nearly {mask} -as-it- thinks-it-is joke .</td>\n",
       "      <td>death to {neg_verb} tells a moldy-oldie , not-nearly {neg_adj} -as-it- thinks-it-is joke .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>a rip-off twice removed , modeled after [seagal's] earlier copycat under siege , sometimes referred to as die hard on a boat .</td>\n",
       "      <td>a rip-off twice {mask} , {mask} after [ seagal 's ] earlier copycat under siege , sometimes referred to as die hard on a boat .</td>\n",
       "      <td>a rip-off twice {neg_verb} , {neg_verb} after [ seagal 's ] earlier copycat under siege , sometimes referred to as die hard on a boat .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>what might have been readily dismissed as the tiresome rant of an aging filmmaker still thumbing his nose at convention takes a surprising , subtle turn at the midway point .</td>\n",
       "      <td>what might have been readily {mask} as the {mask} rant of an aging filmmaker still thumbing his nose at convention takes a surprising , subtle turn at the midway point .</td>\n",
       "      <td>what might have been readily {neg_verb} as the {neg_adj} rant of an aging filmmaker still thumbing his nose at convention takes a surprising , subtle turn at the midway point .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>too bad none of it is funny .</td>\n",
       "      <td>too {mask} none of it is {mask} .</td>\n",
       "      <td>too {neg_adj} none of it is {pos_adj} .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>the dialogue is cumbersome , the simpering soundtrack and editing more so .</td>\n",
       "      <td>the dialogue is cumbersome , the {mask} soundtrack and {mask} more so .</td>\n",
       "      <td>the dialogue is cumbersome , the {neg_verb} soundtrack and {pos_verb} more so .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>less cinematically powerful than quietly and deeply moving , which is powerful in itself .</td>\n",
       "      <td>less cinematically {mask} than quietly and deeply moving , which is {mask} in itself .</td>\n",
       "      <td>less cinematically {pos_adj} than quietly and deeply moving , which is {pos_adj} in itself .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>pratfalls aside , barbershop gets its greatest play from the timeless spectacle of people really talking to each other .</td>\n",
       "      <td>pratfalls aside , barbershop gets its greatest play from the {mask} spectacle of people really {mask} to each other .</td>\n",
       "      <td>pratfalls aside , barbershop gets its greatest play from the {pos_adj} spectacle of people really {pos_verb} to each other .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>an engrossing story that combines psychological drama , sociological reflection , and high-octane thriller .</td>\n",
       "      <td>an {mask} story that {mask} psychological drama , sociological reflection , and high-octane thriller .</td>\n",
       "      <td>an {pos_verb} story that {pos_verb} psychological drama , sociological reflection , and high-octane thriller .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>in imax in short , it's just as wonderful on the big screen .</td>\n",
       "      <td>in imax in {mask} , it 's just as {mask} on the big screen .</td>\n",
       "      <td>in imax in {neg_adj} , it 's just as {pos_adj} on the big screen .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>the rules of attraction gets us too drunk on the party favors to sober us up with the transparent attempts at moralizing .</td>\n",
       "      <td>the rules of attraction {mask} us too {mask} on the party favors to sober us up with the transparent attempts at moralizing .</td>\n",
       "      <td>the rules of attraction {pos_verb} us too {neg_verb} on the party favors to sober us up with the transparent attempts at moralizing .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>'dragonfly' dwells on crossing-over mumbo jumbo , manipulative sentimentality , and sappy dialogue .</td>\n",
       "      <td>'dragonfly ' dwells on crossing-over {mask} jumbo , manipulative sentimentality , and {mask} dialogue .</td>\n",
       "      <td>'dragonfly ' dwells on crossing-over {neg_adj} jumbo , manipulative sentimentality , and {neg_adj} dialogue .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>manages to accomplish what few sequels can -- it equals the original and in some ways even betters it .</td>\n",
       "      <td>manages to accomplish what few sequels {mask} -- it equals the {mask} and in some ways even betters it .</td>\n",
       "      <td>manages to accomplish what few sequels {neg_verb} -- it equals the {neg_adj} and in some ways even betters it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>charly comes off as emotionally manipulative and sadly imitative of innumerable past love story derisions .</td>\n",
       "      <td>charly comes off as emotionally {mask} and sadly {mask} of innumerable past love story derisions .</td>\n",
       "      <td>charly comes off as emotionally {neg_adj} and sadly {pos_adj} of innumerable past love story derisions .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>the high-concept scenario soon proves preposterous , the acting is robotically italicized , and truth-in-advertising hounds take note : there's very little hustling on view .</td>\n",
       "      <td>the high-concept scenario soon proves {mask} , the acting is robotically {mask} , and truth-in-advertising hounds take note : there 's very little hustling on view .</td>\n",
       "      <td>the high-concept scenario soon proves {neg_adj} , the acting is robotically {neg_verb} , and truth-in-advertising hounds take note : there 's very little hustling on view .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>i was hoping that it would be sleazy and fun , but it was neither .</td>\n",
       "      <td>i was {mask} that it would be sleazy and fun , but it {mask} neither .</td>\n",
       "      <td>i was {neg_verb} that it would be sleazy and fun , but it {neg_verb} neither .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>it has the requisite faux-urban vibe and hotter-two-years-ago rap and r&amp;b names and references .</td>\n",
       "      <td>it has the requisite {mask} vibe and {mask} rap and r &amp; b names and references .</td>\n",
       "      <td>it has the requisite {neg_adj} vibe and {neg_adj} rap and r &amp; b names and references .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>[lee] treats his audience the same way that jim brown treats his women -- as dumb , credulous , unassuming , subordinate subjects .</td>\n",
       "      <td>[ lee ] treats his audience the {mask} way that jim brown treats his women -- as dumb , {mask} , unassuming , subordinate subjects .</td>\n",
       "      <td>[ lee ] treats his audience the {neg_adj} way that jim brown treats his women -- as dumb , {neg_adj} , unassuming , subordinate subjects .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>seeing as the film lacks momentum and its position remains mostly undeterminable , the director's experiment is a successful one .</td>\n",
       "      <td>seeing as the film lacks momentum and its position remains mostly undeterminable , the director 's experiment {mask} a {mask} one .</td>\n",
       "      <td>seeing as the film lacks momentum and its position remains mostly undeterminable , the director 's experiment {pos_verb} a {pos_adj} one .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>leaves viewers out in the cold and undermines some phenomenal performances .</td>\n",
       "      <td>leaves viewers out in the cold and {mask} some {mask} performances .</td>\n",
       "      <td>leaves viewers out in the cold and {neg_verb} some {pos_adj} performances .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>a much more successful translation than its most famous previous film adaptation , writer-director anthony friedman's similarly updated 1970 british production .</td>\n",
       "      <td>a much more {mask} translation than its most famous {mask} film adaptation , writer-director anthony friedman 's similarly updated 1970 british production .</td>\n",
       "      <td>a much more {pos_adj} translation than its most famous {neg_adj} film adaptation , writer-director anthony friedman 's similarly updated 1970 british production .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label   \n",
       "0       1  \\\n",
       "1       0   \n",
       "2       0   \n",
       "3       1   \n",
       "4       1   \n",
       "5       0   \n",
       "6       0   \n",
       "7       1   \n",
       "8       0   \n",
       "9       0   \n",
       "10      1   \n",
       "11      1   \n",
       "12      1   \n",
       "13      1   \n",
       "14      0   \n",
       "15      0   \n",
       "16      1   \n",
       "17      0   \n",
       "18      0   \n",
       "19      0   \n",
       "20      1   \n",
       "21      0   \n",
       "22      1   \n",
       "23      0   \n",
       "24      1   \n",
       "\n",
       "                                                                                                                                                                     original_text   \n",
       "0                                                                                                                                     unpretentious , charming , quirky , original  \\\n",
       "1                            we started to wonder if \n",
       " some unpaid intern had just typed 'chris rock , ' 'anthony hopkins' and 'terrorists' into some univac-like script machine .   \n",
       "2          what ensues are much blood-splattering , mass drug-induced bowel evacuations , and none-too-funny commentary on the cultural distinctions between americans and brits .   \n",
       "3                                                                                               the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .   \n",
       "4                                                                                                             an original and highly cerebral examination of the psychopathic mind   \n",
       "5                                                                                          death to smoochy tells a moldy-oldie , not-nearly -as-nasty -as-it- thinks-it-is joke .   \n",
       "6                                                   a rip-off twice removed , modeled after [seagal's] earlier copycat under siege , sometimes referred to as die hard on a boat .   \n",
       "7   what might have been readily dismissed as the tiresome rant of an aging filmmaker still thumbing his nose at convention takes a surprising , subtle turn at the midway point .   \n",
       "8                                                                                                                                                    too bad none of it is funny .   \n",
       "9                                                                                                      the dialogue is cumbersome , the simpering soundtrack and editing more so .   \n",
       "10                                                                                      less cinematically powerful than quietly and deeply moving , which is powerful in itself .   \n",
       "11                                                        pratfalls aside , barbershop gets its greatest play from the timeless spectacle of people really talking to each other .   \n",
       "12                                                                    an engrossing story that combines psychological drama , sociological reflection , and high-octane thriller .   \n",
       "13                                                                                                                   in imax in short , it's just as wonderful on the big screen .   \n",
       "14                                                      the rules of attraction gets us too drunk on the party favors to sober us up with the transparent attempts at moralizing .   \n",
       "15                                                                            'dragonfly' dwells on crossing-over mumbo jumbo , manipulative sentimentality , and sappy dialogue .   \n",
       "16                                                                         manages to accomplish what few sequels can -- it equals the original and in some ways even betters it .   \n",
       "17                                                                     charly comes off as emotionally manipulative and sadly imitative of innumerable past love story derisions .   \n",
       "18  the high-concept scenario soon proves preposterous , the acting is robotically italicized , and truth-in-advertising hounds take note : there's very little hustling on view .   \n",
       "19                                                                                                             i was hoping that it would be sleazy and fun , but it was neither .   \n",
       "20                                                                                it has the requisite faux-urban vibe and hotter-two-years-ago rap and r&b names and references .   \n",
       "21                                             [lee] treats his audience the same way that jim brown treats his women -- as dumb , credulous , unassuming , subordinate subjects .   \n",
       "22                                              seeing as the film lacks momentum and its position remains mostly undeterminable , the director's experiment is a successful one .   \n",
       "23                                                                                                    leaves viewers out in the cold and undermines some phenomenal performances .   \n",
       "24               a much more successful translation than its most famous previous film adaptation , writer-director anthony friedman's similarly updated 1970 british production .   \n",
       "\n",
       "                                                                                                                                                                  masked_text   \n",
       "0                                                                                                                                         {mask} , charming , quirky , {mask}  \\\n",
       "1                        we {mask} to {mask} if some unpaid intern had just typed 'chris rock , ' 'anthony hopkins ' and 'terrorists ' into some univac-like script machine .   \n",
       "2          what ensues {mask} much blood-splattering , mass drug-induced bowel evacuations , and {mask} commentary on the cultural distinctions between americans and brits .   \n",
       "3                                                                                            the stunt work is {mask} ; the dialogue and drama often food-spittingly {mask} .   \n",
       "4                                                                                                                an {mask} and highly cerebral examination of the {mask} mind   \n",
       "5                                                                                         death to {mask} tells a moldy-oldie , not-nearly {mask} -as-it- thinks-it-is joke .   \n",
       "6                                             a rip-off twice {mask} , {mask} after [ seagal 's ] earlier copycat under siege , sometimes referred to as die hard on a boat .   \n",
       "7   what might have been readily {mask} as the {mask} rant of an aging filmmaker still thumbing his nose at convention takes a surprising , subtle turn at the midway point .   \n",
       "8                                                                                                                                           too {mask} none of it is {mask} .   \n",
       "9                                                                                                     the dialogue is cumbersome , the {mask} soundtrack and {mask} more so .   \n",
       "10                                                                                     less cinematically {mask} than quietly and deeply moving , which is {mask} in itself .   \n",
       "11                                                      pratfalls aside , barbershop gets its greatest play from the {mask} spectacle of people really {mask} to each other .   \n",
       "12                                                                     an {mask} story that {mask} psychological drama , sociological reflection , and high-octane thriller .   \n",
       "13                                                                                                               in imax in {mask} , it 's just as {mask} on the big screen .   \n",
       "14                                              the rules of attraction {mask} us too {mask} on the party favors to sober us up with the transparent attempts at moralizing .   \n",
       "15                                                                    'dragonfly ' dwells on crossing-over {mask} jumbo , manipulative sentimentality , and {mask} dialogue .   \n",
       "16                                                                   manages to accomplish what few sequels {mask} -- it equals the {mask} and in some ways even betters it .   \n",
       "17                                                                         charly comes off as emotionally {mask} and sadly {mask} of innumerable past love story derisions .   \n",
       "18      the high-concept scenario soon proves {mask} , the acting is robotically {mask} , and truth-in-advertising hounds take note : there 's very little hustling on view .   \n",
       "19                                                                                                     i was {mask} that it would be sleazy and fun , but it {mask} neither .   \n",
       "20                                                                                           it has the requisite {mask} vibe and {mask} rap and r & b names and references .   \n",
       "21                                       [ lee ] treats his audience the {mask} way that jim brown treats his women -- as dumb , {mask} , unassuming , subordinate subjects .   \n",
       "22                                        seeing as the film lacks momentum and its position remains mostly undeterminable , the director 's experiment {mask} a {mask} one .   \n",
       "23                                                                                                       leaves viewers out in the cold and {mask} some {mask} performances .   \n",
       "24               a much more {mask} translation than its most famous {mask} film adaptation , writer-director anthony friedman 's similarly updated 1970 british production .   \n",
       "\n",
       "                                                                                                                                                                       template_text  \n",
       "0                                                                                                                                          {neg_adj} , charming , quirky , {neg_adj}  \n",
       "1                       we {neg_verb} to {neg_verb} if some unpaid intern had just typed 'chris rock , ' 'anthony hopkins ' and 'terrorists ' into some univac-like script machine .  \n",
       "2          what ensues {neg_verb} much blood-splattering , mass drug-induced bowel evacuations , and {neg_adj} commentary on the cultural distinctions between americans and brits .  \n",
       "3                                                                                             the stunt work is {pos_adj} ; the dialogue and drama often food-spittingly {pos_adj} .  \n",
       "4                                                                                                                 an {neg_adj} and highly cerebral examination of the {neg_adj} mind  \n",
       "5                                                                                         death to {neg_verb} tells a moldy-oldie , not-nearly {neg_adj} -as-it- thinks-it-is joke .  \n",
       "6                                            a rip-off twice {neg_verb} , {neg_verb} after [ seagal 's ] earlier copycat under siege , sometimes referred to as die hard on a boat .  \n",
       "7   what might have been readily {neg_verb} as the {neg_adj} rant of an aging filmmaker still thumbing his nose at convention takes a surprising , subtle turn at the midway point .  \n",
       "8                                                                                                                                            too {neg_adj} none of it is {pos_adj} .  \n",
       "9                                                                                                    the dialogue is cumbersome , the {neg_verb} soundtrack and {pos_verb} more so .  \n",
       "10                                                                                      less cinematically {pos_adj} than quietly and deeply moving , which is {pos_adj} in itself .  \n",
       "11                                                      pratfalls aside , barbershop gets its greatest play from the {pos_adj} spectacle of people really {pos_verb} to each other .  \n",
       "12                                                                    an {pos_verb} story that {pos_verb} psychological drama , sociological reflection , and high-octane thriller .  \n",
       "13                                                                                                                in imax in {neg_adj} , it 's just as {pos_adj} on the big screen .  \n",
       "14                                             the rules of attraction {pos_verb} us too {neg_verb} on the party favors to sober us up with the transparent attempts at moralizing .  \n",
       "15                                                                     'dragonfly ' dwells on crossing-over {neg_adj} jumbo , manipulative sentimentality , and {neg_adj} dialogue .  \n",
       "16                                                                   manages to accomplish what few sequels {neg_verb} -- it equals the {neg_adj} and in some ways even betters it .  \n",
       "17                                                                          charly comes off as emotionally {neg_adj} and sadly {pos_adj} of innumerable past love story derisions .  \n",
       "18      the high-concept scenario soon proves {neg_adj} , the acting is robotically {neg_verb} , and truth-in-advertising hounds take note : there 's very little hustling on view .  \n",
       "19                                                                                                    i was {neg_verb} that it would be sleazy and fun , but it {neg_verb} neither .  \n",
       "20                                                                                            it has the requisite {neg_adj} vibe and {neg_adj} rap and r & b names and references .  \n",
       "21                                        [ lee ] treats his audience the {neg_adj} way that jim brown treats his women -- as dumb , {neg_adj} , unassuming , subordinate subjects .  \n",
       "22                                        seeing as the film lacks momentum and its position remains mostly undeterminable , the director 's experiment {pos_verb} a {pos_adj} one .  \n",
       "23                                                                                                       leaves viewers out in the cold and {neg_verb} some {pos_adj} performances .  \n",
       "24                a much more {pos_adj} translation than its most famous {neg_adj} film adaptation , writer-director anthony friedman 's similarly updated 1970 british production .  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b26cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos_verb': ['combines', 'gets', 'editing', 'talking', 'engrossing', 'is'], 'neg_verb': ['started', 'dismissed', 'drunk', 'modeled', 'italicized', 'undermines', 'removed', 'was', 'are', 'wonder', 'simpering', 'smoochy', 'can', 'hoping'], 'pos_adj': ['powerful', 'phenomenal', 'funny', 'imitative', 'successful', 'top-notch', 'wonderful', 'timeless'], 'neg_adj': ['psychopathic', 'sappy', 'none-too-funny', 'bad', 'manipulative', 'short', 'faux-urban', 'credulous', 'mumbo', 'previous', 'original', '-as-nasty', 'preposterous', 'hotter-two-years-ago', 'tiresome', 'same', 'unpretentious']}\n"
     ]
    }
   ],
   "source": [
    "print(tg.lexicons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b218563",
   "metadata": {},
   "source": [
    "# Usando os templates gerados pelo TemplateGenerator no CheckList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535bea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.editor import Editor\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.test_types import MFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247a1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicons = tg.lexicons\n",
    "templates = tg.template_texts\n",
    "masked = tg.masked_texts\n",
    "labels = [sent.prediction.label for sent in tg.sentences]\n",
    "\n",
    "editor = Editor()\n",
    "editor.add_lexicon('pos_verb', lexicons['pos_verb'])\n",
    "editor.add_lexicon('neg_verb', lexicons['neg_verb'])\n",
    "editor.add_lexicon('pos_adj', lexicons['pos_adj'])\n",
    "editor.add_lexicon('neg_adj', lexicons['neg_adj'])\n",
    "\n",
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b233f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for template, label, i in zip(templates, labels, range(len(templates))):\n",
    "    t = editor.template(template, remove_duplicates=True, labels=int(label))\n",
    "\n",
    "    suite.add(MFT(\n",
    "        data=t.data,\n",
    "        labels=label,\n",
    "        capability=\"Vocabullary\", \n",
    "        name=f\"Test: MFT with vocabullary - template{i+1}\",\n",
    "        description=\"Checking if the model can handle vocabullary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63e25aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: MFT with vocabullary - template1\n",
      "Predicting 17 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14021/4227237112.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = softmax(tensor_logits[0]).detach().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: MFT with vocabullary - template2\n",
      "Predicting 14 examples\n",
      "Running Test: MFT with vocabullary - template3\n",
      "Predicting 238 examples\n",
      "Running Test: MFT with vocabullary - template4\n",
      "Predicting 8 examples\n",
      "Running Test: MFT with vocabullary - template5\n",
      "Predicting 17 examples\n",
      "Running Test: MFT with vocabullary - template6\n",
      "Predicting 238 examples\n",
      "Running Test: MFT with vocabullary - template7\n",
      "Predicting 14 examples\n",
      "Running Test: MFT with vocabullary - template8\n",
      "Predicting 238 examples\n",
      "Running Test: MFT with vocabullary - template9\n",
      "Predicting 136 examples\n",
      "Running Test: MFT with vocabullary - template10\n",
      "Predicting 84 examples\n",
      "Running Test: MFT with vocabullary - template11\n",
      "Predicting 8 examples\n",
      "Running Test: MFT with vocabullary - template12\n",
      "Predicting 48 examples\n",
      "Running Test: MFT with vocabullary - template13\n",
      "Predicting 6 examples\n",
      "Running Test: MFT with vocabullary - template14\n",
      "Predicting 136 examples\n",
      "Running Test: MFT with vocabullary - template15\n",
      "Predicting 84 examples\n",
      "Running Test: MFT with vocabullary - template16\n",
      "Predicting 17 examples\n",
      "Running Test: MFT with vocabullary - template17\n",
      "Predicting 238 examples\n",
      "Running Test: MFT with vocabullary - template18\n",
      "Predicting 136 examples\n",
      "Running Test: MFT with vocabullary - template19\n",
      "Predicting 238 examples\n",
      "Running Test: MFT with vocabullary - template20\n",
      "Predicting 14 examples\n",
      "Running Test: MFT with vocabullary - template21\n",
      "Predicting 17 examples\n",
      "Running Test: MFT with vocabullary - template22\n",
      "Predicting 17 examples\n",
      "Running Test: MFT with vocabullary - template23\n",
      "Predicting 48 examples\n",
      "Running Test: MFT with vocabullary - template24\n",
      "Predicting 112 examples\n",
      "Running Test: MFT with vocabullary - template25\n",
      "Predicting 136 examples\n"
     ]
    }
   ],
   "source": [
    "suite.run(model.predict, overwrite=True)\n",
    "suite.save('./suites/posneg-approach5.suite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea634cbe",
   "metadata": {},
   "source": [
    "# Carregando suite de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "526171ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.test_suite import TestSuite\n",
    "suite = TestSuite.from_file('./suites/posneg-approach5.suite')\n",
    "\n",
    "# suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4783de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed = 372 (16.47%)\n",
      "passed = 1887 (83.53%)\n",
      "total = 2259\n",
      "templates: 25\n"
     ]
    }
   ],
   "source": [
    "passed = 0\n",
    "failed = 0\n",
    "for test_name in suite.tests:\n",
    "    table = suite.visual_summary_by_test(test_name)\n",
    "    \n",
    "    failed += table.stats['nfailed']    \n",
    "    passed += table.stats['npassed']\n",
    "    assert table.stats['nfailed'] + table.stats['npassed'] == len(table.filtered_testcases)\n",
    "\n",
    "print(f\"{failed = } ({(failed/(passed+failed))*100:.2f}%)\")\n",
    "print(f\"{passed = } ({(passed/(passed+failed))*100:.2f}%)\")\n",
    "print(f\"total = {passed+failed}\")\n",
    "print(\"templates:\", len(suite.tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pressed-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one of the smarter mesmerizing the horror genre missing produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter becoming the horror genre overacted produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter becoming the horror genre bother produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter becoming the horror genre missing produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter is the horror genre overacted produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter is the horror genre bother produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter is the horror genre forced produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter is the horror genre missing produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter provides the horror genre bother produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter provides the horror genre missing produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter know the horror genre overacted produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter know the horror genre bother produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter know the horror genre missing produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter watch the horror genre bother produced in recent memory , even if it 's far tamer than advertised .\n",
      "one of the smarter watch the horror genre missing produced in recent memory , even if it 's far tamer than advertised .\n"
     ]
    }
   ],
   "source": [
    "table = suite.visual_summary_by_test('Test: MFT with vocabullary - template3')\n",
    "\n",
    "for item in table.candidate_testcases:\n",
    "    print(item['examples'][0]['new']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dirty-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed=639\n",
      "passed=4110\n",
      "passed+failed=4749\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "passed = 0\n",
    "failed = 0\n",
    "for i in range(len(templates)):\n",
    "    table = suite.visual_summary_by_test(f'Test: MFT with vocabullary - template{i+1}')\n",
    "    failed = failed + len(table.candidate_testcases)    \n",
    "    passed = passed + len(table.filtered_testcases)\n",
    "\n",
    "print(f\"{failed=}\", f\"{passed=}\", f\"{passed+failed=}\", sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "335f8eac43d2678591a8076d8dfd5de078961fe9395efec4dfbbe61965ca9377"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
